{"nbformat":4,"nbformat_minor":0,"metadata":{"environment":{"name":"pytorch-gpu.1-4.m48","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m48"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"SAGAN_CIFAR10.ipynb","provenance":[{"file_id":"1hRPlW-_hvD1oSY-SSI_O7Jnl1tfQJw8k","timestamp":1623836447823}]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a75eacd9765c4955bd23100e995e93c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f72c45eeb918460f87a625dcd1fdd613","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_beaa5dd451e94bafa38ff23726b1ce0b","IPY_MODEL_73c387c382f84887804c125025272a05"]}},"f72c45eeb918460f87a625dcd1fdd613":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"beaa5dd451e94bafa38ff23726b1ce0b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_566482e23eca4aa085b1cd283dc92b82","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_466b532d08c9475dbd980dabbee89693"}},"73c387c382f84887804c125025272a05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_39d27f3a185746bb91863edf82a4cf67","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [11:26&lt;00:00, 248194.95it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a7bd9b00ac4442a2aa61644ee6dbd735"}},"566482e23eca4aa085b1cd283dc92b82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"466b532d08c9475dbd980dabbee89693":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"39d27f3a185746bb91863edf82a4cf67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a7bd9b00ac4442a2aa61644ee6dbd735":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"-bSOJXEYmI0F","executionInfo":{"status":"ok","timestamp":1623836779804,"user_tz":-480,"elapsed":4024,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}}},"source":["import torch\n","from torch.optim.optimizer import Optimizer, required\n","\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from torch import nn\n","from torch import Tensor\n","from torch.nn import Parameter\n","\n","def l2normalize(v, eps=1e-12):\n","    return v / (v.norm() + eps)\n","\n","\n","class SpectralNorm(nn.Module):\n","    def __init__(self, module, name='weight', power_iterations=1):\n","        super(SpectralNorm, self).__init__()\n","        self.module = module\n","        self.name = name\n","        self.power_iterations = power_iterations\n","        if not self._made_params():\n","            self._make_params()\n","\n","    def _update_u_v(self):\n","        u = getattr(self.module, self.name + \"_u\")\n","        v = getattr(self.module, self.name + \"_v\")\n","        w = getattr(self.module, self.name + \"_bar\")\n","\n","        height = w.data.shape[0]\n","        for _ in range(self.power_iterations):\n","            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n","            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n","\n","        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n","        sigma = u.dot(w.view(height, -1).mv(v))\n","        setattr(self.module, self.name, w / sigma.expand_as(w))\n","\n","    def _made_params(self):\n","        try:\n","            u = getattr(self.module, self.name + \"_u\")\n","            v = getattr(self.module, self.name + \"_v\")\n","            w = getattr(self.module, self.name + \"_bar\")\n","            return True\n","        except AttributeError:\n","            return False\n","\n","\n","    def _make_params(self):\n","        w = getattr(self.module, self.name)\n","\n","        height = w.data.shape[0]\n","        width = w.view(height, -1).data.shape[1]\n","\n","        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n","        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n","        u.data = l2normalize(u.data)\n","        v.data = l2normalize(v.data)\n","        w_bar = Parameter(w.data)\n","\n","        del self.module._parameters[self.name]\n","\n","        self.module.register_parameter(self.name + \"_u\", u)\n","        self.module.register_parameter(self.name + \"_v\", v)\n","        self.module.register_parameter(self.name + \"_bar\", w_bar)\n","\n","\n","    def forward(self, *args):\n","        self._update_u_v()\n","        return self.module.forward(*args)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThJ2qE0ml5Rm","executionInfo":{"status":"ok","timestamp":1623836787535,"user_tz":-480,"elapsed":271,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import numpy as np\n","\n","class Self_Attn(nn.Module):\n","    \"\"\" Self attention Layer\"\"\"\n","    def __init__(self, in_dim):\n","        super().__init__()\n","        \n","        # Construct the module\n","        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)\n","        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)\n","        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n","        \n","        self.gamma = nn.Parameter(torch.zeros(1))\n","        self.softmax  = nn.Softmax(dim=-1)\n","        \n","    def forward(self,x):\n","        \"\"\"\n","            inputs :\n","                x : input feature maps( B * C * W * H)\n","            returns :\n","                out : self attention value + input feature \n","                attention: B * N * N (N is Width*Height)\n","        \"\"\"\n","        m_batchsize,C,width ,height = x.size()\n","        \n","        proj_query  = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0,2,1) # B * N * C\n","        proj_key =  self.key_conv(x).view(m_batchsize, -1, width*height) # B * C * N\n","        energy =  torch.bmm(proj_query, proj_key) # batch matrix-matrix product\n","        \n","        attention = self.softmax(energy) # B * N * N\n","        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height) # B * C * N\n","        out = torch.bmm(proj_value, attention.permute(0,2,1)) # batch matrix-matrix product\n","        out = out.view(m_batchsize,C,width,height) # B * C * W * H\n","        \n","        out = self.gamma*out + x\n","        return out, attention\n","\n","class Generator(nn.Module):\n","    \"\"\"\n","    Generator\n","    input: \n","        z: latent matrix with shape of (batch_size, 100)\n","    output: \n","        out: generated image with shape (batch_size, 1, 64, 64)\n","        p1: attention matrix generated by attn layer\n","    \"\"\"\n","    def __init__(self, batch_size=64, attn=True, image_size=64, z_dim=100, conv_dim=64):\n","        super().__init__()\n","        self.attn = attn\n","        \n","        # Layer 1 turn 100 dims -> 512 dims, size 1 -> 4\n","        layer1 = []\n","        layer1.append(SpectralNorm(nn.ConvTranspose2d(in_channels = z_dim, out_channels = conv_dim*8, kernel_size = 4)))\n","        layer1.append(nn.BatchNorm2d(conv_dim*8))\n","        layer1.append(nn.ReLU())\n","        self.l1 = nn.Sequential(*layer1)\n","        \n","        # Layer 2 turn 512 dims -> 256 dims, size 4 -> 8\n","        layer2 = []\n","        layer2.append(SpectralNorm(nn.ConvTranspose2d(in_channels = conv_dim*8, out_channels = conv_dim*4, \n","                                                      kernel_size = 4, stride = 2, padding = 1)))\n","        layer2.append(nn.BatchNorm2d(conv_dim*4))\n","        layer2.append(nn.ReLU())\n","        self.l2 = nn.Sequential(*layer2)\n","        \n","        # Layer 3 turn 256 dims -> 128 dims, size 8 -> 16\n","        layer3 = []\n","        layer3.append(SpectralNorm(nn.ConvTranspose2d(in_channels = conv_dim*4, out_channels = conv_dim*2, \n","                                                      kernel_size = 4, stride = 2, padding = 1)))\n","        layer3.append(nn.BatchNorm2d(conv_dim*2))\n","        layer3.append(nn.ReLU())\n","        self.l3 = nn.Sequential(*layer3)\n","\n","        # Attn1 layer turn 128 dims -> 128 dims\n","        self.attn1 = Self_Attn(conv_dim*2)\n","        \n","        # Layer 4 turn 128 dims -> 64 dims, size 16 -> 32\n","        layer4 = []\n","        layer4.append(SpectralNorm(nn.ConvTranspose2d(in_channels = conv_dim*2, out_channels = conv_dim, \n","                                                      kernel_size = 4, stride = 2, padding = 1)))\n","        layer4.append(nn.BatchNorm2d(conv_dim))\n","        layer4.append(nn.ReLU())\n","        self.l4 = nn.Sequential(*layer4)\n","        \n","        # Attn2 layer turn 64 dims -> 64 dims\n","        self.attn2 = Self_Attn(conv_dim)\n","        \n","        # Layer 5 turn 64 dims -> 3 dims, size 32 -> 64\n","        layer5 = []\n","        layer5.append(nn.ConvTranspose2d(conv_dim, 3, 4, 2, 1))\n","        layer5.append(nn.Tanh())\n","        self.l5 = nn.Sequential(*layer5)\n","        \n","\n","    def forward(self, z):\n","        # z is the input random matrix for generator\n","        z = z.view(z.size(0), z.size(1), 1, 1)\n","        out=self.l1(z)\n","        out=self.l2(out)\n","        out=self.l3(out)\n","        if self.attn == True:\n","            out,_ = self.attn1(out)\n","        out=self.l4(out)\n","        if self.attn == True:\n","            out,_ = self.attn2(out)\n","        out=self.l5(out)\n","\n","        return out\n","\n","\n","class Discriminator(nn.Module):\n","    \"\"\"\n","    Discriminator\n","    input:\n","        x: one batch of data with shape of (batch_size, 1, 64, 64)\n","    output: \n","        out.squeeze: a batch of scalars indicating the predict results\n","        p1: attention matrix generated by attn layer\n","    \"\"\"\n","    def __init__(self, batch_size=64, attn=True, image_size=64, conv_dim=64):\n","        super().__init__()\n","        self.attn = attn\n","        \n","        # Layer 1 turn 3 dims -> 64 dims, size 64 -> 32\n","        layer1 = []\n","        layer1.append(SpectralNorm(nn.Conv2d(3, conv_dim, 4, 2, 1)))\n","        layer1.append(nn.LeakyReLU(0.1))\n","        curr_dim = conv_dim\n","        self.l1 = nn.Sequential(*layer1)\n","        \n","        # Layer 2 turn 64 dims -> 128 dims, size 32 -> 16\n","        layer2 = []\n","        layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n","        layer2.append(nn.LeakyReLU(0.1))\n","        curr_dim = curr_dim * 2\n","        self.l2 = nn.Sequential(*layer2)\n","        \n","        # Layer 3 turn 128 dims -> 256 dims, size 16 -> 8\n","        layer3 = []\n","        layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n","        layer3.append(nn.LeakyReLU(0.1))\n","        curr_dim = curr_dim * 2\n","        self.l3 = nn.Sequential(*layer3)\n","        \n","        # Attn1 layer remains the same dim and size\n","        self.attn1 = Self_Attn(curr_dim)\n","        \n","        # Layer 4 turn 256 dims -> 512 dims, size 8 -> 4\n","        layer4 = []\n","        layer4.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n","        layer4.append(nn.LeakyReLU(0.1))\n","        curr_dim = curr_dim * 2\n","        self.l4 = nn.Sequential(*layer4)\n","        \n","        # Attn2 layer remains the same dim and size\n","        self.attn2 = Self_Attn(curr_dim)\n","        \n","        # Layer 5 turn 512 dims -> 1 dims, size 4 -> 1\n","        layer5 = []\n","        layer5.append(nn.Conv2d(curr_dim, 1, 4, 1, 0))\n","        self.l5 = nn.Sequential(*layer5)\n","\n","    def forward(self, x):\n","        out = self.l1(x)\n","        out = self.l2(out)\n","        out = self.l3(out)\n","        if self.attn == True:\n","            out,_ = self.attn1(out)\n","        out = self.l4(out)\n","        if self.attn == True:\n","            out,_ = self.attn2(out)\n","        out = self.l5(out)\n","\n","        return out.squeeze()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hsMTh0flwSD","executionInfo":{"status":"ok","timestamp":1623836792357,"user_tz":-480,"elapsed":292,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}}},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from torchvision.datasets import ImageFolder\n","from torchvision.datasets import CIFAR10\n","from IPython.display import clear_output\n","import datetime\n","import time\n","import os"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":100,"referenced_widgets":["a75eacd9765c4955bd23100e995e93c5","f72c45eeb918460f87a625dcd1fdd613","beaa5dd451e94bafa38ff23726b1ce0b","73c387c382f84887804c125025272a05","566482e23eca4aa085b1cd283dc92b82","466b532d08c9475dbd980dabbee89693","39d27f3a185746bb91863edf82a4cf67","a7bd9b00ac4442a2aa61644ee6dbd735"]},"id":"OSlbLQZUlwSG","executionInfo":{"status":"ok","timestamp":1623836932856,"user_tz":-480,"elapsed":16255,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}},"outputId":"4cddeaf0-cd9a-4633-99f9-968f280b6306"},"source":["batch_size = 64\n","\n","# Utility functions\n","def cuda(data):\n","    if torch.cuda.is_available():\n","        return data.cuda()\n","    else:\n","        return data\n","\n","def denorm(x):\n","    out = (x + 1) / 2\n","    return out.clamp_(0, 1)\n","\n","img_transform = transforms.Compose([transforms.Resize(64),\n","                                transforms.CenterCrop(64),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize([0.5]*3,[0.5]*3)])\n","\n","dataset = CIFAR10(root='./data', download=True, train=True, transform=img_transform)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","\n","# Fix a random latent input for samples\n","fixed_z = cuda(torch.randn(64, 100))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a75eacd9765c4955bd23100e995e93c5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ./data/cifar-10-python.tar.gz to ./data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BcuoIEUgmfzC","executionInfo":{"status":"ok","timestamp":1623836855093,"user_tz":-480,"elapsed":30258,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}},"outputId":"6a1d5994-3cda-4b8b-fbf9-c2465c45c588"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hgAX4XsqlwSH","executionInfo":{"status":"ok","timestamp":1623837756221,"user_tz":-480,"elapsed":285,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}}},"source":["def train(steps = 100000, batch_size = 64, z_dim = 100, attn = True):\n","    # Initialize model\n","    G = cuda(Generator(batch_size, attn))\n","    D = cuda(Discriminator(batch_size, attn))\n","    \n","    # Make directory for samples and models\n","    cwd = os.getcwd()\n","    post='_attn' if attn else ''\n","    if not os.path.exists(cwd+'/samples_celeba'+post):\n","        os.makedirs(cwd+'/samples_celeba'+post)\n","\n","    # Initialize optimizer with filter, lr and coefficients\n","    g_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, G.parameters()), 0.0001, [0.0,0.9])\n","    d_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), 0.0004, [0.0,0.9])\n","    \n","    # Load data\n","    Iter = iter(dataloader)\n","    \n","    # Start timer\n","    start_time = time.time()\n","    \n","    for step in range(steps):\n","        # ================== Train D ================== #\n","        D.train(); G.train()\n","        try:\n","            real_images,_ = next(Iter)\n","        except:\n","            Iter = iter(dataloader)\n","            real_images,_ = next(Iter)\n","        \n","        # Compute loss with real images\n","        d_out_real = D(cuda(real_images))\n","        d_loss_real = torch.nn.ReLU()(1.0 - d_out_real).mean()\n","        \n","        # Compute loss with fake images\n","        z = cuda(torch.randn(batch_size, z_dim))\n","        fake_images = G(z)\n","        d_out_fake = D(fake_images)\n","        d_loss_fake = torch.nn.ReLU()(1.0 + d_out_fake).mean()\n","        \n","        # Backward + Optimize\n","        d_loss = d_loss_real + d_loss_fake\n","        d_optimizer.zero_grad(); g_optimizer.zero_grad()\n","        d_loss.backward()\n","        d_optimizer.step()\n","        \n","        # ================== Train G ================== #\n","        # Create random noise\n","        z = cuda(torch.randn(batch_size, z_dim))\n","        fake_images = G(z)\n","        g_out_fake = D(fake_images)\n","        g_loss_fake = - g_out_fake.mean()\n","        d_optimizer.zero_grad(); g_optimizer.zero_grad()\n","        g_loss_fake.backward()\n","        g_optimizer.step()\n","        \n","        # Print out log info\n","        if (step + 1) % 10 == 0:\n","            elapsed = time.time() - start_time\n","            expect = elapsed/(step + 1)*(steps-step-1)\n","            elapsed = str(datetime.timedelta(seconds=elapsed))\n","            expect = str(datetime.timedelta(seconds=expect))\n","            clear_output(wait=True)\n","            print(\"Elapsed [{}], Expect [{}], step [{}/{}], D_real_loss: {:.4f}, \"\n","                  \" ave_generator_gamma1: {:.4f}, ave_generator_gamma2: {:.4f}\".\n","                  format(elapsed,expect,step + 1,steps,d_loss_real.item(),\n","                         G.attn1.gamma.mean().item(),\n","                         G.attn2.gamma.mean().item()))\n","        \n","        # Sample images\n","        if (step + 1) % (100) == 0:\n","            fake_images= G(fixed_z)\n","            save_image(denorm(fake_images), os.path.join('/content/drive/My Drive/Colab Notebooks2/SAGAN/samples_cifar10', '{}_fake.png'.format(step + 1)))\n","        \n","        # Save models\n","        #if (step+1) % (100) == 0:\n","            #torch.save(G.state_dict(),os.path.join('./models', '{}_G.pth'.format(step + 1)))\n","            #torch.save(D.state_dict(),os.path.join('./models', '{}_D.pth'.format(step + 1)))"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wttzmFLQlwSI","executionInfo":{"status":"ok","timestamp":1623852027249,"user_tz":-480,"elapsed":14269152,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}},"outputId":"07591ed5-76f0-4c2f-b017-441ad0f4bb5d"},"source":["train(steps = 50000, attn = True)\n","print('Done training part 1')\n","#train(steps = 50000, attn = False)\n","#print('Done training part 2')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Elapsed [3:57:48.240745], Expect [0:00:00], step [50000/50000], D_real_loss: 0.0619,  ave_generator_gamma1: 0.1539, ave_generator_gamma2: -0.1291\n","Done training part 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sy3bPK0klwSO"},"source":["### Generate gif files"]},{"cell_type":"code","metadata":{"id":"dUVFK0_ClwSP","executionInfo":{"status":"ok","timestamp":1623852123554,"user_tz":-480,"elapsed":3101,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}}},"source":["from PIL import Image, ImageDraw, ImageFont\n","\n","font = ImageFont.truetype(\"/content/drive/My Drive/Colab Notebooks2/SAGAN/arial.ttf\", 18)\n","def create_image_with_text(img, wh, text):\n","    width, height = wh\n","    draw = ImageDraw.Draw(img)\n","    draw.text((width, height), text, font = font, fill=\"white\")\n","    return img\n","\n","frames = []\n","\n","for i in range(500, 50001, 500):\n","    #img = Image.open('/content/drive/My Drive/Colab Notebooks2/SAGAN/samples_celeba/{}_fake.png'.format(str(i)))\n","    img1 = Image.open('/content/drive/My Drive/Colab Notebooks2/SAGAN/samples_cifar10/{}_fake.png'.format(str(i)))\n","    width, height = img1.size\n","    expand = Image.new(img1.mode, (width, height), \"black\")\n","    expand.paste(img1, (0, 0))\n","    #expand.paste(img1, (width + 10, 0))\n","    #epoch = round(i*64/202600,2)\n","    #new_frame = create_image_with_text(expand,(10,546), \"After \"+str(epoch)+\" epoches\")\n","    #new_frame = create_image_with_text(new_frame,(10,526), \"Without Attention\")\n","    #new_frame = create_image_with_text(new_frame,(width + 20,526), \"With Attention\")\n","    frames.append(expand)\n","    \n","frames[0].save('/content/drive/My Drive/Colab Notebooks2/SAGAN/gifs_cifar10/sagan_cifar10.gif', format='GIF',\n","               append_images=frames[1:],\n","               save_all=True,\n","               duration=80, loop=0)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXERJaCHlwSQ"},"source":[""],"execution_count":null,"outputs":[]}]}