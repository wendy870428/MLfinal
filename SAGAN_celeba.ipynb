{"nbformat":4,"nbformat_minor":0,"metadata":{"environment":{"name":"pytorch-gpu.1-4.m48","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m48"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"SAGAN_celeba4000.ipynb","provenance":[{"file_id":"1hRPlW-_hvD1oSY-SSI_O7Jnl1tfQJw8k","timestamp":1623847912537}]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-bSOJXEYmI0F","executionInfo":{"status":"ok","timestamp":1623852273440,"user_tz":-480,"elapsed":4103,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}}},"source":["import torch\n","from torch.optim.optimizer import Optimizer, required\n","\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from torch import nn\n","from torch import Tensor\n","from torch.nn import Parameter\n","\n","def l2normalize(v, eps=1e-12):\n","    return v / (v.norm() + eps)\n","\n","\n","class SpectralNorm(nn.Module):\n","    def __init__(self, module, name='weight', power_iterations=1):\n","        super(SpectralNorm, self).__init__()\n","        self.module = module\n","        self.name = name\n","        self.power_iterations = power_iterations\n","        if not self._made_params():\n","            self._make_params()\n","\n","    def _update_u_v(self):\n","        u = getattr(self.module, self.name + \"_u\")\n","        v = getattr(self.module, self.name + \"_v\")\n","        w = getattr(self.module, self.name + \"_bar\")\n","\n","        height = w.data.shape[0]\n","        for _ in range(self.power_iterations):\n","            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n","            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n","\n","        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n","        sigma = u.dot(w.view(height, -1).mv(v))\n","        setattr(self.module, self.name, w / sigma.expand_as(w))\n","\n","    def _made_params(self):\n","        try:\n","            u = getattr(self.module, self.name + \"_u\")\n","            v = getattr(self.module, self.name + \"_v\")\n","            w = getattr(self.module, self.name + \"_bar\")\n","            return True\n","        except AttributeError:\n","            return False\n","\n","\n","    def _make_params(self):\n","        w = getattr(self.module, self.name)\n","\n","        height = w.data.shape[0]\n","        width = w.view(height, -1).data.shape[1]\n","\n","        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n","        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n","        u.data = l2normalize(u.data)\n","        v.data = l2normalize(v.data)\n","        w_bar = Parameter(w.data)\n","\n","        del self.module._parameters[self.name]\n","\n","        self.module.register_parameter(self.name + \"_u\", u)\n","        self.module.register_parameter(self.name + \"_v\", v)\n","        self.module.register_parameter(self.name + \"_bar\", w_bar)\n","\n","\n","    def forward(self, *args):\n","        self._update_u_v()\n","        return self.module.forward(*args)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThJ2qE0ml5Rm","executionInfo":{"status":"ok","timestamp":1623852275033,"user_tz":-480,"elapsed":313,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import numpy as np\n","\n","class Self_Attn(nn.Module):\n","    \"\"\" Self attention Layer\"\"\"\n","    def __init__(self, in_dim):\n","        super().__init__()\n","        \n","        # Construct the module\n","        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)\n","        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)\n","        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n","        \n","        self.gamma = nn.Parameter(torch.zeros(1))\n","        self.softmax  = nn.Softmax(dim=-1)\n","        \n","    def forward(self,x):\n","        \"\"\"\n","            inputs :\n","                x : input feature maps( B * C * W * H)\n","            returns :\n","                out : self attention value + input feature \n","                attention: B * N * N (N is Width*Height)\n","        \"\"\"\n","        m_batchsize,C,width ,height = x.size()\n","        \n","        proj_query  = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0,2,1) # B * N * C\n","        proj_key =  self.key_conv(x).view(m_batchsize, -1, width*height) # B * C * N\n","        energy =  torch.bmm(proj_query, proj_key) # batch matrix-matrix product\n","        \n","        attention = self.softmax(energy) # B * N * N\n","        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height) # B * C * N\n","        out = torch.bmm(proj_value, attention.permute(0,2,1)) # batch matrix-matrix product\n","        out = out.view(m_batchsize,C,width,height) # B * C * W * H\n","        \n","        out = self.gamma*out + x\n","        return out, attention\n","\n","class Generator(nn.Module):\n","    \"\"\"\n","    Generator\n","    input: \n","        z: latent matrix with shape of (batch_size, 100)\n","    output: \n","        out: generated image with shape (batch_size, 1, 64, 64)\n","        p1: attention matrix generated by attn layer\n","    \"\"\"\n","    def __init__(self, batch_size=64, attn=True, image_size=64, z_dim=100, conv_dim=64):\n","        super().__init__()\n","        self.attn = attn\n","        \n","        # Layer 1 turn 100 dims -> 512 dims, size 1 -> 4\n","        layer1 = []\n","        layer1.append(SpectralNorm(nn.ConvTranspose2d(in_channels = z_dim, out_channels = conv_dim*8, kernel_size = 4)))\n","        layer1.append(nn.BatchNorm2d(conv_dim*8))\n","        layer1.append(nn.ReLU())\n","        self.l1 = nn.Sequential(*layer1)\n","        \n","        # Layer 2 turn 512 dims -> 256 dims, size 4 -> 8\n","        layer2 = []\n","        layer2.append(SpectralNorm(nn.ConvTranspose2d(in_channels = conv_dim*8, out_channels = conv_dim*4, \n","                                                      kernel_size = 4, stride = 2, padding = 1)))\n","        layer2.append(nn.BatchNorm2d(conv_dim*4))\n","        layer2.append(nn.ReLU())\n","        self.l2 = nn.Sequential(*layer2)\n","        \n","        # Layer 3 turn 256 dims -> 128 dims, size 8 -> 16\n","        layer3 = []\n","        layer3.append(SpectralNorm(nn.ConvTranspose2d(in_channels = conv_dim*4, out_channels = conv_dim*2, \n","                                                      kernel_size = 4, stride = 2, padding = 1)))\n","        layer3.append(nn.BatchNorm2d(conv_dim*2))\n","        layer3.append(nn.ReLU())\n","        self.l3 = nn.Sequential(*layer3)\n","\n","        # Attn1 layer turn 128 dims -> 128 dims\n","        self.attn1 = Self_Attn(conv_dim*2)\n","        \n","        # Layer 4 turn 128 dims -> 64 dims, size 16 -> 32\n","        layer4 = []\n","        layer4.append(SpectralNorm(nn.ConvTranspose2d(in_channels = conv_dim*2, out_channels = conv_dim, \n","                                                      kernel_size = 4, stride = 2, padding = 1)))\n","        layer4.append(nn.BatchNorm2d(conv_dim))\n","        layer4.append(nn.ReLU())\n","        self.l4 = nn.Sequential(*layer4)\n","        \n","        # Attn2 layer turn 64 dims -> 64 dims\n","        self.attn2 = Self_Attn(conv_dim)\n","        \n","        # Layer 5 turn 64 dims -> 3 dims, size 32 -> 64\n","        layer5 = []\n","        layer5.append(nn.ConvTranspose2d(conv_dim, 3, 4, 2, 1))\n","        layer5.append(nn.Tanh())\n","        self.l5 = nn.Sequential(*layer5)\n","        \n","\n","    def forward(self, z):\n","        # z is the input random matrix for generator\n","        z = z.view(z.size(0), z.size(1), 1, 1)\n","        out=self.l1(z)\n","        out=self.l2(out)\n","        out=self.l3(out)\n","        if self.attn == True:\n","            out,_ = self.attn1(out)\n","        out=self.l4(out)\n","        if self.attn == True:\n","            out,_ = self.attn2(out)\n","        out=self.l5(out)\n","\n","        return out\n","\n","\n","class Discriminator(nn.Module):\n","    \"\"\"\n","    Discriminator\n","    input:\n","        x: one batch of data with shape of (batch_size, 1, 64, 64)\n","    output: \n","        out.squeeze: a batch of scalars indicating the predict results\n","        p1: attention matrix generated by attn layer\n","    \"\"\"\n","    def __init__(self, batch_size=64, attn=True, image_size=64, conv_dim=64):\n","        super().__init__()\n","        self.attn = attn\n","        \n","        # Layer 1 turn 3 dims -> 64 dims, size 64 -> 32\n","        layer1 = []\n","        layer1.append(SpectralNorm(nn.Conv2d(3, conv_dim, 4, 2, 1)))\n","        layer1.append(nn.LeakyReLU(0.1))\n","        curr_dim = conv_dim\n","        self.l1 = nn.Sequential(*layer1)\n","        \n","        # Layer 2 turn 64 dims -> 128 dims, size 32 -> 16\n","        layer2 = []\n","        layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n","        layer2.append(nn.LeakyReLU(0.1))\n","        curr_dim = curr_dim * 2\n","        self.l2 = nn.Sequential(*layer2)\n","        \n","        # Layer 3 turn 128 dims -> 256 dims, size 16 -> 8\n","        layer3 = []\n","        layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n","        layer3.append(nn.LeakyReLU(0.1))\n","        curr_dim = curr_dim * 2\n","        self.l3 = nn.Sequential(*layer3)\n","        \n","        # Attn1 layer remains the same dim and size\n","        self.attn1 = Self_Attn(curr_dim)\n","        \n","        # Layer 4 turn 256 dims -> 512 dims, size 8 -> 4\n","        layer4 = []\n","        layer4.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n","        layer4.append(nn.LeakyReLU(0.1))\n","        curr_dim = curr_dim * 2\n","        self.l4 = nn.Sequential(*layer4)\n","        \n","        # Attn2 layer remains the same dim and size\n","        self.attn2 = Self_Attn(curr_dim)\n","        \n","        # Layer 5 turn 512 dims -> 1 dims, size 4 -> 1\n","        layer5 = []\n","        layer5.append(nn.Conv2d(curr_dim, 1, 4, 1, 0))\n","        self.l5 = nn.Sequential(*layer5)\n","\n","    def forward(self, x):\n","        out = self.l1(x)\n","        out = self.l2(out)\n","        out = self.l3(out)\n","        if self.attn == True:\n","            out,_ = self.attn1(out)\n","        out = self.l4(out)\n","        if self.attn == True:\n","            out,_ = self.attn2(out)\n","        out = self.l5(out)\n","\n","        return out.squeeze()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hsMTh0flwSD","executionInfo":{"status":"ok","timestamp":1623852278505,"user_tz":-480,"elapsed":422,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}}},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from torchvision.datasets import ImageFolder\n","from torchvision.datasets import CIFAR10\n","from IPython.display import clear_output\n","import datetime\n","import time\n","import os"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BcuoIEUgmfzC","executionInfo":{"status":"ok","timestamp":1623860289852,"user_tz":-480,"elapsed":43563,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}},"outputId":"af5b9590-12cc-41b7-c739-3b2296d74f8c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OSlbLQZUlwSG","executionInfo":{"status":"ok","timestamp":1623852340369,"user_tz":-480,"elapsed":31921,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}},"outputId":"b3f25b3e-65af-4896-943d-bd42f40f2462"},"source":["batch_size = 64\n","\n","# Utility functions\n","def cuda(data):\n","    if torch.cuda.is_available():\n","        return data.cuda()\n","    else:\n","        return data\n","\n","def denorm(x):\n","    out = (x + 1) / 2\n","    return out.clamp_(0, 1)\n","\n","transform = transforms.Compose([transforms.Resize(64),\n","                                transforms.CenterCrop(64),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize([0.5]*3,[0.5]*3)])\n","\n","train_data = ImageFolder('/content/drive/My Drive/Colab Notebooks2/SAGAN/gifs_celeba4000', transform=transform)\n","\n","dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=4)\n","\n","# Fix a random latent input for samples\n","fixed_z = cuda(torch.randn(64, 100))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hgAX4XsqlwSH","executionInfo":{"status":"ok","timestamp":1623860219780,"user_tz":-480,"elapsed":426,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}}},"source":["def train(steps = 100000, batch_size = 64, z_dim = 100, attn = True):\n","    # Initialize model\n","    G = cuda(Generator(batch_size, attn))\n","    D = cuda(Discriminator(batch_size, attn))\n","    \n","    # Make directory for samples and models\n","    cwd = os.getcwd()\n","    post='_attn' if attn else ''\n","    if not os.path.exists(cwd+'/samples_celeba'+post):\n","        os.makedirs(cwd+'/samples_celeba'+post)\n","\n","    # Initialize optimizer with filter, lr and coefficients\n","    g_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, G.parameters()), 0.0001, [0.0,0.9])\n","    d_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), 0.0004, [0.0,0.9])\n","    \n","    # Load data\n","    Iter = iter(dataloader)\n","    \n","    # Start timer\n","    start_time = time.time()\n","    \n","    for step in range(steps):\n","        # ================== Train D ================== #\n","        D.train(); G.train()\n","        try:\n","            real_images,_ = next(Iter)\n","        except:\n","            Iter = iter(dataloader)\n","            real_images,_ = next(Iter)\n","        \n","        # Compute loss with real images\n","        d_out_real = D(cuda(real_images))\n","        d_loss_real = torch.nn.ReLU()(1.0 - d_out_real).mean()\n","        \n","        # Compute loss with fake images\n","        z = cuda(torch.randn(batch_size, z_dim))\n","        fake_images = G(z)\n","        d_out_fake = D(fake_images)\n","        d_loss_fake = torch.nn.ReLU()(1.0 + d_out_fake).mean()\n","        \n","        # Backward + Optimize\n","        d_loss = d_loss_real + d_loss_fake\n","        d_optimizer.zero_grad(); g_optimizer.zero_grad()\n","        d_loss.backward()\n","        d_optimizer.step()\n","        \n","        # ================== Train G ================== #\n","        # Create random noise\n","        z = cuda(torch.randn(batch_size, z_dim))\n","        fake_images = G(z)\n","        g_out_fake = D(fake_images)\n","        g_loss_fake = - g_out_fake.mean()\n","        d_optimizer.zero_grad(); g_optimizer.zero_grad()\n","        g_loss_fake.backward()\n","        g_optimizer.step()\n","        \n","        # Print out log info\n","        if (step + 1) % 10 == 0:\n","            elapsed = time.time() - start_time\n","            expect = elapsed/(step + 1)*(steps-step-1)\n","            elapsed = str(datetime.timedelta(seconds=elapsed))\n","            expect = str(datetime.timedelta(seconds=expect))\n","            clear_output(wait=True)\n","            print(\"Elapsed [{}], Expect [{}], step [{}/{}], D_real_loss: {:.4f}, \"\n","                  \" ave_generator_gamma1: {:.4f}, ave_generator_gamma2: {:.4f}\".\n","                  format(elapsed,expect,step + 1,steps,d_loss_real.item(),\n","                         G.attn1.gamma.mean().item(),\n","                         G.attn2.gamma.mean().item()))\n","        \n","        # Sample images\n","        if (step + 1) % (200) == 0:\n","            fake_images= G(fixed_z)\n","            save_image(denorm(fake_images), os.path.join('/content/drive/My Drive/Colab Notebooks2/SAGAN/samples_celeba4000', '{}_fake.png'.format(step + 1)))\n","        \n","        # Save models\n","        #if (step+1) % (100) == 0:\n","            #torch.save(G.state_dict(),os.path.join('./models', '{}_G.pth'.format(step + 1)))\n","            #torch.save(D.state_dict(),os.path.join('./models', '{}_D.pth'.format(step + 1)))"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wttzmFLQlwSI","outputId":"90a913bd-d8af-4a2e-fc6d-b2b7ada7c036"},"source":["train(steps = 20000, attn = True)\n","print('Done training part 1')\n","#train(steps = 20000, attn = False)\n","#print('Done training part 2')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Elapsed [1:38:55.119023], Expect [0:01:33.442707], step [19690/20000], D_real_loss: 0.0317,  ave_generator_gamma1: 0.1627, ave_generator_gamma2: -0.1456\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sy3bPK0klwSO"},"source":["### Generate gif files"]},{"cell_type":"code","metadata":{"id":"dUVFK0_ClwSP","executionInfo":{"status":"ok","timestamp":1623860333157,"user_tz":-480,"elapsed":38366,"user":{"displayName":"Wenhsien Ho","photoUrl":"","userId":"04340698061193725186"}}},"source":["from PIL import Image, ImageDraw, ImageFont\n","\n","font = ImageFont.truetype(\"/content/drive/My Drive/Colab Notebooks2/SAGAN/arial.ttf\", 18)\n","def create_image_with_text(img, wh, text):\n","    width, height = wh\n","    draw = ImageDraw.Draw(img)\n","    draw.text((width, height), text, font = font, fill=\"white\")\n","    return img\n","\n","frames = []\n","\n","for i in range(200, 19601, 200):\n","    #img = Image.open('/content/drive/My Drive/Colab Notebooks/SAGAN/samples_celeba/{}_fake.png'.format(str(i)))\n","    img1 = Image.open('/content/drive/My Drive/Colab Notebooks2/SAGAN/samples_celeba4000/{}_fake.png'.format(str(i)))\n","    width, height = img1.size\n","    expand = Image.new(img1.mode, (width, height), \"black\")\n","    expand.paste(img1, (0, 0))\n","    #expand.paste(img1, (width + 10, 0))\n","    epoch = round(i*64/202600,2)\n","    #new_frame = create_image_with_text(expand,(10,546), \"After \"+str(epoch)+\" epoches\")\n","    #new_frame = create_image_with_text(new_frame,(10,526), \"Without Attention\")\n","    #new_frame = create_image_with_text(new_frame,(width + 20,526), \"With Attention\")\n","    frames.append(expand)\n","    \n","frames[0].save('/content/drive/My Drive/Colab Notebooks2/SAGAN/gifs_celeba4000/sagan_celebA4000.gif', format='GIF',\n","               append_images=frames[1:],\n","               save_all=True,\n","               duration=80, loop=0)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXERJaCHlwSQ"},"source":[""],"execution_count":null,"outputs":[]}]}